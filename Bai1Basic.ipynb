{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9f33b9-41f6-4df2-ba66-bf346c0ab264",
   "metadata": {},
   "source": [
    "#  Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6abbc-ea2a-443c-80d0-fba3cb51aff1",
   "metadata": {},
   "source": [
    "![Image](https://machinelearningmastery.com/wp-content/uploads/2022/03/dotproduct_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b618576-1eec-48b3-a2c8-39d676301eb2",
   "metadata": {},
   "source": [
    "1. https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras\n",
    "2. https://learning.rasa.com/transformers/kvq/\n",
    "3. https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/\n",
    "4. https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32869462-d7e6-4f3e-9ea3-1bece36d86ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 5, 7, 9, 5, 3)\n",
      "(9, 5, 7, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones([9, 5, 7, 4])\n",
    "c = np.ones([9, 5, 4, 3])\n",
    "print(np.dot(a, c).shape)\n",
    "print(np.matmul(a, c).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8e0dc-0ea8-42b9-80be-215d0cc4d264",
   "metadata": {},
   "source": [
    "## matmul in numpys\n",
    "matmul(x1, x2, /, out=None, *, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
    "\n",
    "Matrix product of two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ed9bd7-496c-4b95-877f-8e81571b1454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([9, 5, 7, 4])\n",
    "c = np.array([9, 5, 4, 3])\n",
    "np.matmul(a,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d0c51c-165a-4fc4-bc1e-5e489849807f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 0],\n",
    "              [0, 1]])\n",
    "\n",
    "b = np.array([[4, 1],\n",
    "              [2, 2]])\n",
    "\n",
    "np.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172f490-2bc0-4bae-857b-7a94ba2146f7",
   "metadata": {},
   "source": [
    "## Cast in tensorflow:\n",
    "Casts a tensor to a new type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0c7e76-07c1-4171-8e57-43e36433a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 2])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.constant([3.1, 2.2], dtype=tf.float32)\n",
    "tf.cast(x, tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308e6cd-5e05-41cd-a044-4aabaf481dfd",
   "metadata": {},
   "source": [
    "## Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "810a49c3-1785-4f3d-ac1f-dc1035730b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21194157 0.57611686 0.21194157]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "inp = np.asarray([1., 2., 1.])\n",
    "layer = tf.keras.layers.Softmax()\n",
    "values = layer(inp).numpy()\n",
    "print(values)\n",
    "print(values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9de489a-7e86-41c6-81dc-d39609b835f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.  0.5]\n"
     ]
    }
   ],
   "source": [
    "mask = np.asarray([True, False, True], dtype=bool)\n",
    "print(layer(inp, mask).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d7af1b-e2a9-4d3a-a1d4-73088f4ebd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import matmul, math, cast, float32\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.backend import softmax\n",
    "\n",
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9a89b5-bec8-49d8-97f7-c1e68d911efb",
   "metadata": {},
   "source": [
    "## Exampel customer database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "534b26dd-4976-41ff-b27f-2d93555c4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = DotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a04b8c37-9408-4622-a4cb-573a13252cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random \n",
    "\n",
    "queries = random.random((1, 3, 1))\n",
    "keys = random.random((1, 3, 1))\n",
    "values = random.random((1, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e9a8429-e985-4c2a-8d42-ebaea6b788e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.15150703]\n",
      "  [0.59940113]\n",
      "  [0.62429778]]]\n",
      "[[[0.22485589]\n",
      "  [0.38444304]\n",
      "  [0.67865426]]]\n",
      "[[[0.49856748]\n",
      "  [0.04415472]\n",
      "  [0.60872006]]]\n"
     ]
    }
   ],
   "source": [
    "print(queries)\n",
    "print(keys)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "398b6f1c-19ea-43a5-9d47-4426f080d962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 1), dtype=float32, numpy=\n",
       "array([[[0.3862987 ],\n",
       "        [0.3944249 ],\n",
       "        [0.39490905]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(queries, keys, values, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "809b6f4e-627a-4476-a332-98d6157c0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "batch_size = 64  # Batch size from the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c906359c-a835-4c48-a652-978b6d297273",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee952f1c-feb6-45bb-b701-2bcb62193068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.5611781  0.5592531  0.47254524 ... 0.51013523 0.44942003 0.55581266]\n",
      "  [0.5837234  0.57207894 0.46463716 ... 0.513379   0.45144668 0.5826472 ]\n",
      "  [0.55580467 0.5453951  0.45683342 ... 0.49314564 0.46573904 0.53475696]\n",
      "  [0.57554805 0.5696033  0.4642083  ... 0.5080715  0.4548852  0.5723881 ]\n",
      "  [0.57027376 0.5715811  0.46606666 ... 0.505357   0.45338583 0.56504273]]\n",
      "\n",
      " [[0.5418818  0.6224016  0.59333324 ... 0.5632441  0.64164567 0.33463573]\n",
      "  [0.53591913 0.63183004 0.59787047 ... 0.558737   0.6301525  0.3387793 ]\n",
      "  [0.5596818  0.6237744  0.5706537  ... 0.5420102  0.63351244 0.32748112]\n",
      "  [0.53451145 0.62813604 0.61336714 ... 0.54385847 0.62513834 0.34475732]\n",
      "  [0.54508036 0.6270265  0.58449066 ... 0.55972    0.63672835 0.33292016]]\n",
      "\n",
      " [[0.35427496 0.29653743 0.4464199  ... 0.3617815  0.6000948  0.4925366 ]\n",
      "  [0.3576734  0.28109956 0.43979156 ... 0.36254445 0.60864705 0.49505597]\n",
      "  [0.36143368 0.29390565 0.44873062 ... 0.34816417 0.5888282  0.48878267]\n",
      "  [0.3417774  0.3090346  0.44043967 ... 0.370695   0.6126771  0.48562664]\n",
      "  [0.3536948  0.29417172 0.43289778 ... 0.35009992 0.60681075 0.49379927]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.57661235 0.3399946  0.5206235  ... 0.59395534 0.3942464  0.22505148]\n",
      "  [0.5883545  0.3433715  0.53580105 ... 0.57409847 0.42846096 0.24030265]\n",
      "  [0.583877   0.33898544 0.5315587  ... 0.5744578  0.42723304 0.2388531 ]\n",
      "  [0.58261764 0.33869815 0.5288825  ... 0.5776753  0.42242438 0.2362542 ]\n",
      "  [0.59307283 0.3425748  0.5119386  ... 0.5819797  0.41922167 0.23215762]]\n",
      "\n",
      " [[0.44649944 0.39843085 0.6419471  ... 0.653707   0.60018355 0.45853835]\n",
      "  [0.41577598 0.3882723  0.63747066 ... 0.6219232  0.5892367  0.41265813]\n",
      "  [0.430483   0.39747876 0.6255068  ... 0.62008816 0.59996796 0.42803454]\n",
      "  [0.43447402 0.40643734 0.64432466 ... 0.6406833  0.60544264 0.4487712 ]\n",
      "  [0.43806902 0.4133673  0.65009004 ... 0.6455105  0.6113547  0.4539874 ]]\n",
      "\n",
      " [[0.38558626 0.46799293 0.48200297 ... 0.5304084  0.3482561  0.24698997]\n",
      "  [0.3952809  0.46092042 0.46161154 ... 0.4916991  0.35800081 0.24861589]\n",
      "  [0.40597132 0.46389136 0.47630966 ... 0.518257   0.3472795  0.2430149 ]\n",
      "  [0.40770838 0.45678133 0.47752243 ... 0.49603918 0.33732486 0.23593283]\n",
      "  [0.40167212 0.46667045 0.47310483 ... 0.5396447  0.36672604 0.26001853]]], shape=(64, 5, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attention = DotProductAttention()\n",
    "print(attention(queries, keys, values, d_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794f3b4-bebb-494b-bf6d-b4af01a919fe",
   "metadata": {},
   "source": [
    "## Excersice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
