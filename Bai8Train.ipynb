{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d46baaa-2c9e-4c36-89bd-19ff25837910",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/training-the-transformer-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c19a59-69f4-4766-bc1f-c197b75dc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy.random import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64\n",
    "\n",
    "\n",
    "class PrepareDataset:\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(PrepareDataset, self).__init__(**kwargs)\n",
    "\t\tself.n_sentences = 10000  # Number of sentences to include in the dataset\n",
    "\t\tself.train_split = 0.9  # Ratio of the training data split\n",
    "\n",
    "\t# Fit a tokenizer\n",
    "\tdef create_tokenizer(self, dataset):\n",
    "\t\ttokenizer = Tokenizer()\n",
    "\t\ttokenizer.fit_on_texts(dataset)\n",
    "\n",
    "\t\treturn tokenizer\n",
    "\n",
    "\tdef find_seq_length(self, dataset):\n",
    "\t\treturn max(len(seq.split()) for seq in dataset)\n",
    "\n",
    "\tdef find_vocab_size(self, tokenizer, dataset):\n",
    "\t\ttokenizer.fit_on_texts(dataset)\n",
    "\n",
    "\t\treturn len(tokenizer.word_index) + 1\n",
    "\n",
    "\tdef __call__(self, filename, **kwargs):\n",
    "\t\t# Load a clean dataset\n",
    "\t\tclean_dataset = load(open(filename, 'rb'))\n",
    "\n",
    "\t\t# Reduce dataset size\n",
    "\t\tdataset = clean_dataset[:self.n_sentences, :]\n",
    "\n",
    "\t\t# Include start and end of string tokens\n",
    "\t\tfor i in range(dataset[:, 0].size):\n",
    "\t\t\tdataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n",
    "\t\t\tdataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n",
    "\n",
    "\t\t# Random shuffle the dataset\n",
    "\t\tshuffle(dataset)\n",
    "\n",
    "\t\t# Split the dataset\n",
    "\t\ttrain = dataset[:int(self.n_sentences * self.train_split)]\n",
    "\n",
    "\t\t# Prepare tokenizer for the encoder input\n",
    "\t\tenc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "\t\tenc_seq_length = self.find_seq_length(train[:, 0])\n",
    "\t\tenc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "\t\t# Encode and pad the input sequences\n",
    "\t\ttrainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "\t\ttrainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
    "\t\ttrainX = convert_to_tensor(trainX, dtype=int64)\n",
    "\n",
    "\t\t# Prepare tokenizer for the decoder input\n",
    "\t\tdec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "\t\tdec_seq_length = self.find_seq_length(train[:, 1])\n",
    "\t\tdec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "\n",
    "\t\t# Encode and pad the input sequences\n",
    "\t\ttrainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "\t\ttrainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
    "\t\ttrainY = convert_to_tensor(trainY, dtype=int64)\n",
    "\n",
    "\t\treturn trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291268ed-6620-45db-8e4d-408c704f3e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 12:07:40.366180: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-11 12:07:40.366203: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-11 12:07:40.366225: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3459): /proc/driver/nvidia/version does not exist\n",
      "2023-02-11 12:07:40.366517: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> do kids like you <EOS> \n",
      " tf.Tensor([  1  12 324  34   5   2   0], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training data\n",
    "dataset = PrepareDataset()\n",
    "trainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    "\n",
    "print(train_orig[0, 0], '\\n', trainX[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe04f578-43a5-47bc-8345-156baa71d9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder sequence length: 7\n"
     ]
    }
   ],
   "source": [
    "print('Encoder sequence length:', enc_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587e1d67-9973-4577-960c-23b5e8630e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> mogen kinder dich <EOS> \n",
      " tf.Tensor([  1 966 269  21   2   0   0   0   0   0   0   0], shape=(12,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(train_orig[0, 1], '\\n', trainY[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea685f45-ad92-4b3a-a850-d5018bef09ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder sequence length: 12\n"
     ]
    }
   ],
   "source": [
    "print('Decoder sequence length:', dec_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9494d638-3ce7-4b80-9640-ff079d63dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class TransformerModel(Model):\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n",
    "        super(TransformerModel, self).__init__(**kwargs)\n",
    "\n",
    "        # Set up the encoder\n",
    "        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
    "\n",
    "        # Set up the decoder\n",
    "        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
    "\n",
    "        # Define the final dense layer\n",
    "        self.model_last_layer = Dense(dec_vocab_size)\n",
    "\n",
    "    def padding_mask(self, input):\n",
    "        # Create mask which marks the zero padding values in the input by a 1.0\n",
    "        mask = math.equal(input, 0)\n",
    "        mask = cast(mask, float32)\n",
    "\n",
    "        # The shape of the mask should be broadcastable to the shape\n",
    "        # of the attention weights that it will be masking later on\n",
    "        return mask[:, newaxis, newaxis, :]\n",
    "\n",
    "    def lookahead_mask(self, shape):\n",
    "        # Mask out future entries by marking them with a 1.0\n",
    "        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def call(self, encoder_input, decoder_input, training):\n",
    "\n",
    "        # Create padding mask to mask the encoder inputs and the encoder outputs in the decoder\n",
    "        enc_padding_mask = self.padding_mask(encoder_input)\n",
    "\n",
    "        # Create and combine padding and look-ahead masks to be fed into the decoder\n",
    "        dec_in_padding_mask = self.padding_mask(decoder_input)\n",
    "        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n",
    "        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n",
    "\n",
    "        # Feed the input into the encoder\n",
    "        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n",
    "\n",
    "        # Feed the encoder output into the decoder\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n",
    "\n",
    "        # Pass the decoder output through a final dense layer\n",
    "        model_output = self.model_last_layer(decoder_output)\n",
    "\n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0231f05-df88-4577-b927-d9dd6305c1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Epoch 1 Step 0 Loss 8.3113 Accuracy 0.0000\n",
      "Epoch 1 Step 50 Loss 7.5655 Accuracy 0.1343\n",
      "Epoch 1 Step 100 Loss 6.9635 Accuracy 0.1766\n",
      "Epoch 1: Training Loss 6.6396, Training Accuracy 0.1978\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0 Loss 5.6246 Accuracy 0.2842\n",
      "Epoch 2 Step 50 Loss 5.3761 Accuracy 0.2784\n",
      "Epoch 2 Step 100 Loss 5.2274 Accuracy 0.2851\n",
      "Epoch 2: Training Loss 5.1137, Training Accuracy 0.2901\n",
      "Total time taken: 173.21s\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, TensorSpec, function, int64\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "# from model import TransformerModel\n",
    "# from prepare_dataset import PrepareDataset\n",
    "from time import time\n",
    "\n",
    "\n",
    "# Define the model parameters\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "# Define the training parameters\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1\n",
    "\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "dec_vocab_size = 20 # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 5  # Maximum length of the input sequence\n",
    "dec_seq_length = 5  # Maximum length of the target sequence\n",
    "\n",
    "\n",
    "\n",
    "# Implementing a learning rate scheduler\n",
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super(LRScheduler, self).__init__(**kwargs)\n",
    "\n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "\n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "# Instantiate an Adam optimizer\n",
    "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n",
    "\n",
    "# Prepare the training and test splits of the dataset\n",
    "# dataset = PrepareDataset()\n",
    "trainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    "\n",
    "# Prepare the dataset batches\n",
    "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "\n",
    "# Defining the loss function\n",
    "def loss_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the computation of loss\n",
    "    padding_mask = math.logical_not(equal(target, 0))\n",
    "    padding_mask = cast(padding_mask, float32)\n",
    "\n",
    "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
    "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n",
    "\n",
    "    # Compute the mean loss over the unmasked values\n",
    "    return reduce_sum(loss) / reduce_sum(padding_mask)\n",
    "\n",
    "\n",
    "# Defining the accuracy function\n",
    "def accuracy_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the computation of accuracy\n",
    "    padding_mask = math.logical_not(equal(target, 0))\n",
    "\n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2))\n",
    "    accuracy = math.logical_and(padding_mask, accuracy)\n",
    "\n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    padding_mask = cast(padding_mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    "\n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(padding_mask)\n",
    "\n",
    "\n",
    "# Include metrics monitoring\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = Mean(name='train_accuracy')\n",
    "\n",
    "# Create a checkpoint object and manager to manage multiple checkpoints\n",
    "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
    "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n",
    "\n",
    "# Speeding up the training process\n",
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "\n",
    "        # Run the forward pass of the model to generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "\n",
    "        # Compute the training loss\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "\n",
    "        # Compute the training accuracy\n",
    "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
    "\n",
    "    # Retrieve gradients of the trainable variables with respect to the training loss\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    "\n",
    "    # Update the values of the trainable variables by gradient descent\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    # Iterate over the dataset batches\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    "\n",
    "        # Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = train_batchX[:, 1:]\n",
    "        decoder_input = train_batchY[:, :-1]\n",
    "        decoder_output = train_batchY[:, 1:]\n",
    "\n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "            # print(\"Samples so far: %s\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Print epoch number and loss value at the end of every epoch\n",
    "    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    # Save a checkpoint after every five epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n",
    "\n",
    "print(\"Total time taken: %.2fs\" % (time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a061ad8-3314-4f27-9521-9b534af3a3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
