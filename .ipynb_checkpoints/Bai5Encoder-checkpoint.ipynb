{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1324d3-de2e-43d3-b70f-4d4da09be7f5",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a543fd61-2be6-4478-940c-49d13201482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from positional_encoding import PositionEmbeddingFixedWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc4df8b-9bbb-44e0-9fcf-b8c3d5419813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    " \n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    " \n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45f3407-39d0-4a2c-b359-4528d8efb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    " \n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    " \n",
    "        return self.fully_connected2(self.activation(x_fc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0637cd5f-0a7f-4595-9914-dddac392db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    " \n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    " \n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    " \n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a816dd-7ab5-49e9-a858-e9b1db27daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    " \n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    " \n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    " \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162a7eca-4ed4-4c1c-9dcf-e859fb51526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.539414   -0.49370763 -0.6746148  ... -0.5499159  -1.0280532\n",
      "   -0.04929206]\n",
      "  [ 0.13512793 -0.8923105  -0.9589842  ... -0.2629767  -1.1592865\n",
      "   -0.54286283]\n",
      "  [ 0.19394068 -0.28730956 -1.0302358  ...  0.26369336 -0.7274633\n",
      "    0.3004794 ]\n",
      "  [-0.14933045 -1.4270809  -0.3014332  ... -0.8654095  -1.1721716\n",
      "    0.00383488]\n",
      "  [-0.05292033 -1.4912024  -0.5521967  ... -0.3396967   0.26464412\n",
      "   -0.20398071]]\n",
      "\n",
      " [[ 0.455427   -0.07245557 -0.20347902 ... -0.67836136 -1.105499\n",
      "   -0.03410398]\n",
      "  [ 1.1434654   0.4249746  -0.36399123 ... -0.64332604 -0.8328506\n",
      "   -0.11400229]\n",
      "  [ 0.7404346  -0.929404   -0.10080502 ... -0.67183876 -1.0256205\n",
      "   -0.17701708]\n",
      "  [ 1.4358137  -1.0569617  -0.12098601 ... -0.25784323 -1.4402784\n",
      "   -0.17225198]\n",
      "  [ 0.79270244 -0.81642044 -0.04372903 ... -1.1253201  -0.39774653\n",
      "    0.24588709]]\n",
      "\n",
      " [[ 0.26940197 -1.0913955  -0.642574   ... -1.0060345  -0.03615547\n",
      "   -0.3945781 ]\n",
      "  [ 0.47031668 -0.85363936 -0.9763367  ... -0.80627596 -0.22358589\n",
      "   -0.75918037]\n",
      "  [ 0.4070526  -0.6423074  -0.5248595  ... -1.2008438  -0.40971082\n",
      "   -0.14883758]\n",
      "  [ 0.2206411  -0.76107913 -0.1674312  ... -0.09767652  0.07953417\n",
      "   -0.23054922]\n",
      "  [-0.35112694 -0.3820843  -0.44351882 ... -0.97253954 -0.5294451\n",
      "   -0.64413345]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.38002497 -0.5678759  -0.27456176 ... -0.3728297  -0.5804476\n",
      "   -0.40699154]\n",
      "  [ 0.13692828 -0.39491302 -0.42078307 ...  0.03752749 -0.35298854\n",
      "    0.45887786]\n",
      "  [ 0.71430796 -0.90405375  0.08178046 ... -0.460611   -0.3472533\n",
      "    0.19128248]\n",
      "  [-0.25577796 -0.82142085  0.41141406 ... -0.00585469 -0.4108347\n",
      "    0.12063456]\n",
      "  [ 0.3557821  -0.9921794  -0.48333138 ...  0.00884159 -0.560989\n",
      "   -0.14621274]]\n",
      "\n",
      " [[-0.14597192 -0.6798424  -0.60848707 ... -0.5147722  -0.611698\n",
      "    0.8901057 ]\n",
      "  [ 0.6615165  -0.6342012  -0.6185511  ... -0.6275513  -1.2444566\n",
      "   -0.78606325]\n",
      "  [ 0.0208367  -0.13575539 -0.8226649  ... -1.0586603  -1.2613614\n",
      "   -0.14702265]\n",
      "  [ 0.33736658 -0.05740055 -0.06037783 ... -0.63241875 -1.2761091\n",
      "   -0.07927649]\n",
      "  [-0.5934054  -0.35368353  0.51420754 ... -0.5355892  -1.8334403\n",
      "   -0.00542863]]\n",
      "\n",
      " [[ 0.400238   -1.3864149   0.03389844 ... -0.9664974  -0.6299798\n",
      "    0.53829515]\n",
      "  [ 0.54586995 -0.71981895 -0.8632964  ... -0.6564825  -0.54126143\n",
      "    0.06387591]\n",
      "  [ 0.30579984 -0.78495175 -0.31269753 ... -0.6851812   0.15018445\n",
      "    1.0099003 ]\n",
      "  [ 0.8003627  -1.2900316  -0.48927915 ... -0.75825083 -0.07791645\n",
      "    0.34884778]\n",
      "  [ 0.75408506 -1.5257794  -0.56952846 ... -0.4015549   0.25024086\n",
      "    0.41381407]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    " \n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    " \n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    " \n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    " \n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e272307c-87b3-45d8-ae35-6b4a5f51c68a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
