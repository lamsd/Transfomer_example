{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5987d140-8d51-4e9c-a5f4-1d3015079e8e",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d65ac9-ce99-49f9-9f24-598e75679c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 10:55:39.777906: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-11 10:55:40.020417: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-11 10:55:40.020459: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-11 10:55:40.082399: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-11 10:55:41.270016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-11 10:55:41.270250: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-11 10:55:41.270271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from keras.backend import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0430f92f-04dd-4fa9-a605-215ac79cff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746d8e20-2c08-4dc0-bf74-6aeb78d3b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73596e88-9dc6-44f3-ac94-417b29597a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.08638423  0.53370076  0.12105258 ...  0.18486507 -0.10878904\n",
      "   -0.5566584 ]\n",
      "  [-0.08439093  0.53370357  0.12081177 ...  0.18463941 -0.11065985\n",
      "   -0.5564932 ]\n",
      "  [-0.08335359  0.5336666   0.11998411 ...  0.18306735 -0.10787352\n",
      "   -0.5542876 ]\n",
      "  [-0.0822272   0.533469    0.12070324 ...  0.18278901 -0.10876165\n",
      "   -0.5538137 ]\n",
      "  [-0.08306656  0.53189653  0.12041227 ...  0.18146572 -0.10793507\n",
      "   -0.55644876]]\n",
      "\n",
      " [[ 0.04705728  0.644515    0.0912054  ...  0.1279698   0.03800036\n",
      "   -0.45178849]\n",
      "  [ 0.04927927  0.64380896  0.09248097 ...  0.12950821  0.03764124\n",
      "   -0.45327702]\n",
      "  [ 0.04887827  0.64786386  0.0882956  ...  0.12893943  0.03991658\n",
      "   -0.45145112]\n",
      "  [ 0.04847192  0.6463412   0.08697274 ...  0.12812573  0.0402009\n",
      "   -0.4517158 ]\n",
      "  [ 0.04392612  0.64405036  0.09013927 ...  0.13281274  0.03941422\n",
      "   -0.45234796]]\n",
      "\n",
      " [[-0.02606148  0.7375861   0.07741801 ...  0.10820106  0.01048888\n",
      "   -0.50747854]\n",
      "  [-0.02895715  0.73501015  0.07822878 ...  0.11134184  0.00977601\n",
      "   -0.51160026]\n",
      "  [-0.02892551  0.7380324   0.07923283 ...  0.11118073  0.01021195\n",
      "   -0.5108887 ]\n",
      "  [-0.02762513  0.73646694  0.07837225 ...  0.10892858  0.0107328\n",
      "   -0.5121615 ]\n",
      "  [-0.03090781  0.7380597   0.07749397 ...  0.10982205  0.00976589\n",
      "   -0.5101546 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.02739824  0.6958397   0.09174863 ...  0.28594336  0.0547911\n",
      "   -0.47644728]\n",
      "  [ 0.0263993   0.68936795  0.0913021  ...  0.28263152  0.05311491\n",
      "   -0.47300282]\n",
      "  [ 0.02676542  0.6947335   0.08847757 ...  0.28250062  0.04966925\n",
      "   -0.4756652 ]\n",
      "  [ 0.02619834  0.69383943  0.08782706 ...  0.28364354  0.05175504\n",
      "   -0.47382927]\n",
      "  [ 0.02659592  0.6972145   0.09150255 ...  0.28470576  0.05210164\n",
      "   -0.4765859 ]]\n",
      "\n",
      " [[-0.03894537  0.64358544  0.11306681 ...  0.17150444  0.02622125\n",
      "   -0.3546587 ]\n",
      "  [-0.03633197  0.6454114   0.1123397  ...  0.1699354   0.02695379\n",
      "   -0.35589966]\n",
      "  [-0.03585963  0.6457198   0.11272864 ...  0.17109184  0.02767251\n",
      "   -0.35341185]\n",
      "  [-0.03765497  0.6438265   0.11469265 ...  0.17231506  0.02754394\n",
      "   -0.3548473 ]\n",
      "  [-0.03890908  0.6443682   0.11645134 ...  0.17180651  0.03021592\n",
      "   -0.35397112]]\n",
      "\n",
      " [[ 0.0271401   0.6898644   0.0097321  ...  0.17073065  0.02114997\n",
      "   -0.46660548]\n",
      "  [ 0.02745105  0.69324905  0.01186597 ...  0.17245695  0.02107149\n",
      "   -0.46836007]\n",
      "  [ 0.02450814  0.6917676   0.01012586 ...  0.17122707  0.02218676\n",
      "   -0.46618405]\n",
      "  [ 0.02723998  0.6905757   0.00909619 ...  0.16710484  0.0191389\n",
      "   -0.4669156 ]\n",
      "  [ 0.02734536  0.6935781   0.0115308  ...  0.17265336  0.01951936\n",
      "   -0.4696597 ]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 10:55:42.949534: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-11 10:55:42.949571: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-11 10:55:42.949594: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3459): /proc/driver/nvidia/version does not exist\n",
      "2023-02-11 10:55:42.949850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "batch_size = 64  # Batch size from the training process\n",
    "\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "print(multihead_attention(queries, keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c1ab9-5f22-4e0e-bd77-1472396bceaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
