{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76f5b22a-d7b9-482b-ae8e-bc2946b9cec9",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/implementing-the-transformer-decoder-from-scratch-in-tensorflow-and-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8f54d2-c5af-458a-bf85-7c8995c76f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Dropout\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from positional_encoding import PositionEmbeddingFixedWeights\n",
    "from encoder import AddNormalization, FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4f34e6-5a77-4d2a-a631-76acab0184b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Decoder Layer\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "        self.add_norm3 = AddNormalization()\n",
    "\n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by another multi-head attention layer\n",
    "        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d35703-85a4-4de8-99fe-f54d272acf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Decoder\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.decoder_layer = [DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(output_target)\n",
    "        # Expected output shape = (number of sentences, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.decoder_layer):\n",
    "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0518136a-7c25-453d-95e8-276dbc9e2856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 11:14:12.985254: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-11 11:14:12.985295: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-11 11:14:12.985317: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3459): /proc/driver/nvidia/version does not exist\n",
      "2023-02-11 11:14:12.985584: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-4.86915201e-01  5.09294033e-01  5.53200305e-01 ...  6.17483556e-01\n",
      "   -3.35109308e-02 -8.95759881e-01]\n",
      "  [-4.25104707e-01  4.65781122e-01  6.55828893e-01 ...  5.85942745e-01\n",
      "   -1.77997584e-03 -9.58913207e-01]\n",
      "  [-4.71767277e-01  3.92947584e-01  6.84855461e-01 ...  5.52861392e-01\n",
      "    4.03840989e-02 -1.00801456e+00]\n",
      "  [-5.85143387e-01  3.71899813e-01  6.29702210e-01 ...  5.26565075e-01\n",
      "    3.45119350e-02 -9.99072492e-01]\n",
      "  [-6.84700906e-01  4.43548352e-01  5.29980123e-01 ...  5.34509659e-01\n",
      "   -3.64141422e-04 -9.56187487e-01]]\n",
      "\n",
      " [[-5.90422273e-01  2.36242846e-01  7.85882533e-01 ...  7.00807810e-01\n",
      "    4.09616604e-02 -8.78546178e-01]\n",
      "  [-5.44177473e-01  1.71626046e-01  8.89099658e-01 ...  6.72504902e-01\n",
      "    6.84150308e-02 -9.19992983e-01]\n",
      "  [-5.74525476e-01  6.63944855e-02  9.01148081e-01 ...  6.31177068e-01\n",
      "    9.53117311e-02 -9.55910325e-01]\n",
      "  [-6.48089945e-01  1.93028469e-02  8.14290762e-01 ...  6.30570650e-01\n",
      "    8.42501596e-02 -9.71653819e-01]\n",
      "  [-7.45411873e-01  9.91721377e-02  6.90556109e-01 ...  6.60442352e-01\n",
      "    4.07381393e-02 -9.70723152e-01]]\n",
      "\n",
      " [[-5.85281014e-01  2.41268158e-01  1.01126373e+00 ...  5.20207226e-01\n",
      "   -3.18022877e-01 -9.06793535e-01]\n",
      "  [-5.21332562e-01  1.74847350e-01  1.11158752e+00 ...  5.00790536e-01\n",
      "   -2.97301650e-01 -9.55774844e-01]\n",
      "  [-5.58706641e-01  9.42495987e-02  1.13640630e+00 ...  4.62431431e-01\n",
      "   -2.66215682e-01 -9.76435184e-01]\n",
      "  [-6.54639065e-01  7.93097913e-02  1.07118595e+00 ...  4.55135792e-01\n",
      "   -2.79415667e-01 -9.79570687e-01]\n",
      "  [-7.65863836e-01  1.63218960e-01  9.70452607e-01 ...  4.89603043e-01\n",
      "   -3.16489339e-01 -9.64866340e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-5.40472031e-01  4.17164236e-01  8.67565155e-01 ...  5.65039754e-01\n",
      "    2.48608962e-01 -7.30486512e-01]\n",
      "  [-4.81411010e-01  3.73864621e-01  9.70424294e-01 ...  5.39671779e-01\n",
      "    2.79392838e-01 -7.66926467e-01]\n",
      "  [-5.28830886e-01  3.05849940e-01  9.87820923e-01 ...  4.92963254e-01\n",
      "    3.00781995e-01 -7.85591841e-01]\n",
      "  [-6.45017624e-01  2.86881119e-01  9.15169835e-01 ...  4.81150746e-01\n",
      "    2.84765244e-01 -7.94701755e-01]\n",
      "  [-7.54278362e-01  3.61110806e-01  7.98182487e-01 ...  5.15897393e-01\n",
      "    2.37964705e-01 -7.89365888e-01]]\n",
      "\n",
      " [[-7.94701993e-01  1.94982722e-01  4.42619503e-01 ...  5.95137000e-01\n",
      "   -6.50071204e-02 -1.06769931e+00]\n",
      "  [-7.37973571e-01  1.42784938e-01  5.65766692e-01 ...  5.79368651e-01\n",
      "   -4.24917340e-02 -1.11112571e+00]\n",
      "  [-7.68313885e-01  4.87136878e-02  5.90670884e-01 ...  5.68778515e-01\n",
      "   -1.11839948e-02 -1.13353848e+00]\n",
      "  [-8.60396922e-01  1.12670148e-02  5.15937209e-01 ...  5.64619958e-01\n",
      "   -7.12632900e-03 -1.12828708e+00]\n",
      "  [-9.47142661e-01  1.00419357e-01  4.11671370e-01 ...  5.70620418e-01\n",
      "   -5.21595590e-02 -1.11051548e+00]]\n",
      "\n",
      " [[-4.18026894e-01  3.50539461e-02  9.06836033e-01 ...  8.64257038e-01\n",
      "    4.31024469e-02 -1.04959846e+00]\n",
      "  [-3.54948252e-01 -1.75810289e-02  1.00765598e+00 ...  8.33025873e-01\n",
      "    7.02593252e-02 -1.10045624e+00]\n",
      "  [-4.01131570e-01 -1.09715894e-01  1.03061569e+00 ...  7.92124569e-01\n",
      "    9.15825069e-02 -1.14157963e+00]\n",
      "  [-5.04435420e-01 -1.39820665e-01  9.53152716e-01 ...  7.78571665e-01\n",
      "    8.32840726e-02 -1.16081250e+00]\n",
      "  [-6.22172236e-01 -6.21422902e-02  8.26596737e-01 ...  7.96647191e-01\n",
      "    5.67204244e-02 -1.14866018e+00]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "dec_vocab_size = 20  # Vocabulary size for the decoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the decoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "enc_output = random.random((batch_size, input_seq_length, d_model))\n",
    "\n",
    "decoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(decoder(input_seq, enc_output, None, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15a056-6ab9-4f2d-a6cb-e19ca4006582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
